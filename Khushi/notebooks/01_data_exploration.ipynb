{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil Spill Detection - Data Exploration\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis (EDA) on the oil spill detection dataset.\n",
    "\n",
    "## Objectives\n",
    "1. Load and examine the dataset structure\n",
    "2. Analyze image properties and class distribution\n",
    "3. Visualize sample data\n",
    "4. Calculate dataset statistics\n",
    "5. Identify potential data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "# Fixed import path to correctly reference src directory\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Updated imports to use correct module path\n",
    "try:\n",
    "    from data.data_loader import OilSpillDataLoader, create_sample_dataset\n",
    "    from data.preprocessor import OilSpillPreprocessor\n",
    "    print(\"‚úÖ Custom modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "    print(\"Python path:\", sys.path)\n",
    "    print(\"\\nTrying alternative import...\")\n",
    "    \n",
    "    # Alternative import method\n",
    "    import importlib.util\n",
    "    \n",
    "    # Load data_loader module\n",
    "    spec = importlib.util.spec_from_file_location(\"data_loader\", \"../src/data/data_loader.py\")\n",
    "    data_loader_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(data_loader_module)\n",
    "    \n",
    "    OilSpillDataLoader = data_loader_module.OilSpillDataLoader\n",
    "    create_sample_dataset = data_loader_module.create_sample_dataset\n",
    "    \n",
    "    # Load preprocessor module\n",
    "    spec = importlib.util.spec_from_file_location(\"preprocessor\", \"../src/data/preprocessor.py\")\n",
    "    preprocessor_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(preprocessor_module)\n",
    "    \n",
    "    OilSpillPreprocessor = preprocessor_module.OilSpillPreprocessor\n",
    "    print(\"‚úÖ Alternative import successful!\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated to work with user's actual dataset structure\n",
    "# Define data directory - update this path to your actual dataset location\n",
    "data_dir = \"../dataset\"  # This should point to your dataset folder with train/val/test\n",
    "\n",
    "# Check if user's dataset exists\n",
    "if not os.path.exists(data_dir):\n",
    "    print(f\"‚ùå Dataset not found at {data_dir}\")\n",
    "    print(\"Please update the data_dir path to point to your dataset folder\")\n",
    "    print(\"Expected structure:\")\n",
    "    print(\"dataset/\")\n",
    "    print(\"‚îú‚îÄ‚îÄ train/\")\n",
    "    print(\"‚îú‚îÄ‚îÄ val/\")\n",
    "    print(\"‚îú‚îÄ‚îÄ test/\")\n",
    "    print(\"‚îî‚îÄ‚îÄ label_colors\")\n",
    "    \n",
    "    # Create sample dataset for demonstration\n",
    "    sample_dir = \"../data/raw\"\n",
    "    print(f\"\\nCreating sample dataset at {sample_dir} for demonstration...\")\n",
    "    create_sample_dataset(sample_dir, num_samples=50)\n",
    "    data_dir = sample_dir\n",
    "    print(\"‚úÖ Sample dataset created!\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataset found at {data_dir}\")\n",
    "\n",
    "# Initialize data loader\n",
    "print(\"\\nüîÑ Loading dataset...\")\n",
    "loader = OilSpillDataLoader(data_dir)\n",
    "dataset_info = loader.load_dataset_info()\n",
    "\n",
    "print(\"\\nüìä Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in dataset_info.items():\n",
    "    if key not in ['sample_image_paths', 'sample_mask_paths', 'label_colors_info']:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Show label colors info if available\n",
    "if 'label_colors_info' in dataset_info:\n",
    "    print(f\"\\nüé® Label Colors Info:\")\n",
    "    print(dataset_info['label_colors_info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated visualization to work with both classification and segmentation datasets\n",
    "print(\"üñºÔ∏è Visualizing sample data...\")\n",
    "\n",
    "if dataset_info['dataset_type'] == 'classification':\n",
    "    # Load classification data samples\n",
    "    try:\n",
    "        train_images, train_labels = loader.load_classification_data('train', target_size=(256, 256))\n",
    "        \n",
    "        # Show first 12 samples\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "        \n",
    "        for i in range(min(12, len(train_images))):\n",
    "            row = i // 4\n",
    "            col = i % 4\n",
    "            \n",
    "            axes[row, col].imshow(train_images[i])\n",
    "            label_text = \"Oil Spill\" if train_labels[i] == 1 else \"Clean Water\"\n",
    "            axes[row, col].set_title(f'Sample {i+1}: {label_text}')\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(train_images), 12):\n",
    "            row = i // 4\n",
    "            col = i % 4\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.suptitle('Sample Images from Training Set', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(train_images)} training samples\")\n",
    "        print(f\"üìä Label distribution: {np.bincount(train_labels)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading classification data: {e}\")\n",
    "        print(\"This might be because your dataset structure is different than expected.\")\n",
    "\n",
    "else:\n",
    "    # Original segmentation visualization code\n",
    "    fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
    "    \n",
    "    num_samples = min(6, dataset_info['total_samples'])\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        try:\n",
    "            # Load image and mask\n",
    "            image, mask = loader.load_image_pair(i)\n",
    "            \n",
    "            # Original image\n",
    "            axes[0, i].imshow(image)\n",
    "            axes[0, i].set_title(f'Image {i+1}')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Mask\n",
    "            axes[1, i].imshow(mask, cmap='gray')\n",
    "            axes[1, i].set_title(f'Mask {i+1}')\n",
    "            axes[1, i].axis('off')\n",
    "            \n",
    "            # Overlay\n",
    "            overlay = image.copy()\n",
    "            overlay[:, :, 0] = np.where(mask > 0.5, 1.0, overlay[:, :, 0])  # Red overlay for oil spills\n",
    "            axes[2, i].imshow(overlay)\n",
    "            axes[2, i].set_title(f'Overlay {i+1}')\n",
    "            axes[2, i].axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Statistics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated statistics calculation for classification datasets\n",
    "print(\"üìà Calculating dataset statistics...\")\n",
    "\n",
    "if dataset_info['dataset_type'] == 'classification':\n",
    "    # Analyze all splits\n",
    "    all_stats = {}\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        try:\n",
    "            images, labels = loader.load_classification_data(split, target_size=(256, 256))\n",
    "            \n",
    "            if len(images) > 0:\n",
    "                # Calculate image statistics\n",
    "                image_array = np.array(images)\n",
    "                \n",
    "                split_stats = {\n",
    "                    'num_samples': len(images),\n",
    "                    'num_oil_spill': np.sum(labels),\n",
    "                    'num_clean_water': len(labels) - np.sum(labels),\n",
    "                    'mean_intensity': np.mean(image_array),\n",
    "                    'std_intensity': np.std(image_array),\n",
    "                    'min_intensity': np.min(image_array),\n",
    "                    'max_intensity': np.max(image_array)\n",
    "                }\n",
    "                \n",
    "                all_stats[split] = split_stats\n",
    "                \n",
    "                print(f\"\\nüìä {split.upper()} SET STATISTICS:\")\n",
    "                print(f\"   Total samples: {split_stats['num_samples']}\")\n",
    "                print(f\"   Oil spill samples: {split_stats['num_oil_spill']} ({split_stats['num_oil_spill']/split_stats['num_samples']*100:.1f}%)\")\n",
    "                print(f\"   Clean water samples: {split_stats['num_clean_water']} ({split_stats['num_clean_water']/split_stats['num_samples']*100:.1f}%)\")\n",
    "                print(f\"   Mean intensity: {split_stats['mean_intensity']:.4f}\")\n",
    "                print(f\"   Intensity std: {split_stats['std_intensity']:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analyzing {split} set: {e}\")\n",
    "            continue\n",
    "\n",
    "else:\n",
    "    # Original segmentation statistics\n",
    "    stats = loader.get_sample_statistics(num_samples=min(50, dataset_info['total_samples']))\n",
    "    \n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(f\"Samples analyzed: {stats['sample_count']}\")\n",
    "    print(f\"Average oil spill ratio: {stats['average_spill_ratio']:.4f}\")\n",
    "    print(f\"Spill ratio std: {stats['spill_ratio_std']:.4f}\")\n",
    "    print(f\"Min spill ratio: {stats['min_spill_ratio']:.4f}\")\n",
    "    print(f\"Max spill ratio: {stats['max_spill_ratio']:.4f}\")\n",
    "    print(f\"\\nImage shapes (first 5): {stats['image_shapes'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated visualization for classification datasets\n",
    "print(\"üìä Creating data distribution visualizations...\")\n",
    "\n",
    "if dataset_info['dataset_type'] == 'classification' and 'all_stats' in locals():\n",
    "    # Create comprehensive visualization for classification\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Class distribution across splits\n",
    "    splits = list(all_stats.keys())\n",
    "    oil_counts = [all_stats[split]['num_oil_spill'] for split in splits]\n",
    "    clean_counts = [all_stats[split]['num_clean_water'] for split in splits]\n",
    "    \n",
    "    x = np.arange(len(splits))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 0].bar(x - width/2, oil_counts, width, label='Oil Spill', alpha=0.8)\n",
    "    axes[0, 0].bar(x + width/2, clean_counts, width, label='Clean Water', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Dataset Split')\n",
    "    axes[0, 0].set_ylabel('Number of Samples')\n",
    "    axes[0, 0].set_title('Class Distribution Across Splits')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(splits)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Overall class distribution pie chart\n",
    "    total_oil = sum(oil_counts)\n",
    "    total_clean = sum(clean_counts)\n",
    "    \n",
    "    axes[0, 1].pie([total_oil, total_clean], \n",
    "                   labels=['Oil Spill', 'Clean Water'],\n",
    "                   autopct='%1.1f%%',\n",
    "                   startangle=90)\n",
    "    axes[0, 1].set_title('Overall Class Distribution')\n",
    "    \n",
    "    # Intensity distribution across splits\n",
    "    mean_intensities = [all_stats[split]['mean_intensity'] for split in splits]\n",
    "    std_intensities = [all_stats[split]['std_intensity'] for split in splits]\n",
    "    \n",
    "    axes[0, 2].bar(splits, mean_intensities, alpha=0.7, color='orange')\n",
    "    axes[0, 2].set_title('Mean Intensity by Split')\n",
    "    axes[0, 2].set_ylabel('Mean Intensity')\n",
    "    \n",
    "    # Sample count by split\n",
    "    sample_counts = [all_stats[split]['num_samples'] for split in splits]\n",
    "    axes[1, 0].bar(splits, sample_counts, alpha=0.7, color='green')\n",
    "    axes[1, 0].set_title('Sample Count by Split')\n",
    "    axes[1, 0].set_ylabel('Number of Samples')\n",
    "    \n",
    "    # Class balance visualization\n",
    "    balance_ratios = [all_stats[split]['num_oil_spill']/all_stats[split]['num_samples'] for split in splits]\n",
    "    axes[1, 1].plot(splits, balance_ratios, marker='o', linewidth=2, markersize=8)\n",
    "    axes[1, 1].axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Perfect Balance')\n",
    "    axes[1, 1].set_title('Class Balance Across Splits')\n",
    "    axes[1, 1].set_ylabel('Oil Spill Ratio')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # Intensity standard deviation\n",
    "    axes[1, 2].bar(splits, std_intensities, alpha=0.7, color='purple')\n",
    "    axes[1, 2].set_title('Intensity Std by Split')\n",
    "    axes[1, 2].set_ylabel('Std Intensity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Visualization not available - using sample data or segmentation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated summary for classification datasets\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA EXPLORATION SUMMARY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ Dataset type: {dataset_info['dataset_type']}\")\n",
    "print(f\"   ‚Ä¢ Total samples: {dataset_info.get('total_samples', 'N/A')}\")\n",
    "\n",
    "if dataset_info['dataset_type'] == 'classification':\n",
    "    print(f\"   ‚Ä¢ Training samples: {dataset_info.get('train_samples', 0)}\")\n",
    "    print(f\"   ‚Ä¢ Validation samples: {dataset_info.get('val_samples', 0)}\")\n",
    "    print(f\"   ‚Ä¢ Test samples: {dataset_info.get('test_samples', 0)}\")\n",
    "    \n",
    "    if dataset_info.get('classes'):\n",
    "        print(f\"   ‚Ä¢ Classes: {dataset_info['classes']}\")\n",
    "        if dataset_info.get('class_distribution'):\n",
    "            print(f\"   ‚Ä¢ Class distribution: {dataset_info['class_distribution']}\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDATIONS:\")\n",
    "if dataset_info['dataset_type'] == 'classification':\n",
    "    total_samples = dataset_info.get('total_samples', 0)\n",
    "    if total_samples > 100:\n",
    "        print(f\"   ‚Ä¢ ‚úÖ Dataset size is adequate for training ({total_samples} samples)\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ ‚ö†Ô∏è Dataset is small ({total_samples} samples) - consider data augmentation\")\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Use data augmentation: rotation, flipping, brightness/contrast adjustment\")\n",
    "    print(f\"   ‚Ä¢ Apply normalization and preprocessing\")\n",
    "    print(f\"   ‚Ä¢ Consider transfer learning with pre-trained models\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Segmentation dataset detected\")\n",
    "    print(f\"   ‚Ä¢ Use U-Net or similar architecture for pixel-wise classification\")\n",
    "\n",
    "print(f\"\\n‚úÖ MILESTONE 1 STATUS: DATA EXPLORATION COMPLETE\")\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"   1. Run preprocessing notebook (02_preprocessing.ipynb)\")\n",
    "print(f\"   2. Implement data augmentation\")\n",
    "print(f\"   3. Prepare data loaders for training\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "os.makedirs('../results/figures', exist_ok=True)\n",
    "os.makedirs('../results/data', exist_ok=True)\n",
    "\n",
    "# Save dataset information\n",
    "dataset_info_df = pd.DataFrame([dataset_info])\n",
    "dataset_info_df.to_csv('../results/data/dataset_info.csv', index=False)\n",
    "\n",
    "# Save statistics if available\n",
    "if 'all_stats' in locals():\n",
    "    stats_df = pd.DataFrame(all_stats).T\n",
    "    stats_df.to_csv('../results/data/dataset_statistics.csv')\n",
    "    print(\"üìÅ Classification statistics saved\")\n",
    "\n",
    "print(\"üìÅ Results saved to ../results/ directory\")\n",
    "print(\"\\nüéâ DATA EXPLORATION COMPLETE!\")\n",
    "print(\"Next step: Run preprocessing notebook (02_preprocessing.ipynb)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
