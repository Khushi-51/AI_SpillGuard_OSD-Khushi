{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil Spill Segmentation - Model Development (Minimal)\n",
    "## U-Net Implementation for Binary Segmentation\n",
    "\n",
    "This notebook implements a minimal U-Net model for oil spill segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Essential imports only\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA memory:\", torch.cuda.get_device_properties(0).total_memory // 1024**3, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from dataset pipeline\n",
      "Image size: 256\n",
      "Batch size: 2\n",
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load configuration from dataset pipeline\n",
    "try:\n",
    "    # Load from correct config folder and file name\n",
    "    with open('config/dataset_config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"Loaded configuration from dataset pipeline\")\n",
    "    print(f\"Image size: {config.get('image_size', 'Not found')}\")\n",
    "    print(f\"Batch size: {config.get('batch_size', 'Not found')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load config: {e}\")\n",
    "    # Removed dummy config fallback, require real config\n",
    "    raise Exception(\"Dataset config required! Run dataset pipeline first.\")\n",
    "\n",
    "# Use GPU if available, CPU as fallback\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Architecture Implementation\n",
    "Minimal U-Net with encoder-decoder structure for binary segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net building blocks defined successfully\n"
     ]
    }
   ],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Double convolution block used in U-Net\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # Handle size mismatch\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "print(\"U-Net building blocks defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net model class defined successfully\n"
     ]
    }
   ],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"Minimal U-Net for binary segmentation\"\"\"\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Encoder (downsampling path)\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        \n",
    "        # Decoder (upsampling path)\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        \n",
    "        # Output layer\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        # Output\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "print(\"U-Net model class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing U-Net model...\n",
      "Total parameters: 31,037,633\n",
      "Trainable parameters: 31,037,633\n",
      "Model size: 118.40 MB\n",
      "Model moved to cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "print(\"Initializing U-Net model...\")\n",
    "model = UNet(n_channels=3, n_classes=1)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024**2:.2f} MB\")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "print(f\"Model moved to {device}\")\n",
    "\n",
    "# Memory cleanup\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model with sample input...\n",
      "Test input shape: torch.Size([1, 3, 256, 256])\n",
      "Model output shape: torch.Size([1, 1, 256, 256])\n",
      "Output range: [-0.1246, -0.0601]\n",
      "Probability output range: [0.4689, 0.4850]\n",
      "âœ“ Model test successful!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test model with sample input\n",
    "print(\"Testing model with sample input...\")\n",
    "\n",
    "# Use config values for testing\n",
    "batch_size = 1  # Small batch for testing\n",
    "image_size = config.get('image_size', 256)\n",
    "\n",
    "test_input = torch.randn(batch_size, 3, image_size, image_size).to(device)\n",
    "print(f\"Test input shape: {test_input.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "\n",
    "print(f\"Model output shape: {output.shape}\")\n",
    "print(f\"Output range: [{output.min().item():.4f}, {output.max().item():.4f}]\")\n",
    "\n",
    "# Apply sigmoid for binary segmentation\n",
    "prob_output = torch.sigmoid(output)\n",
    "print(f\"Probability output range: [{prob_output.min().item():.4f}, {prob_output.max().item():.4f}]\")\n",
    "\n",
    "print(\"âœ“ Model test successful!\")\n",
    "\n",
    "# Cleanup\n",
    "del test_input, output, prob_output\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Binary Segmentation\n",
    "Implementing Binary Cross Entropy + Dice Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for binary segmentation\"\"\"\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Apply sigmoid to predictions\n",
    "        predictions = torch.sigmoid(predictions)\n",
    "        \n",
    "        # Flatten tensors\n",
    "        predictions = predictions.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # Calculate Dice coefficient\n",
    "        intersection = (predictions * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Binary Cross Entropy + Dice Loss\"\"\"\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        bce = self.bce_loss(predictions, targets)\n",
    "        dice = self.dice_loss(predictions, targets)\n",
    "        return self.bce_weight * bce + self.dice_weight * dice\n",
    "\n",
    "print(\"Loss functions defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss functions...\n",
      "Prediction shape: torch.Size([1, 1, 256, 256])\n",
      "Target shape: torch.Size([1, 1, 256, 256])\n",
      "BCE Loss: 0.8043\n",
      "Dice Loss: 0.5007\n",
      "Combined Loss: 0.6525\n",
      "âœ“ Loss functions test successful!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test loss functions with real dimensions\n",
    "print(\"Testing loss functions...\")\n",
    "\n",
    "# Use actual config dimensions for testing\n",
    "batch_size = 1\n",
    "image_size = config.get('image_size', 256)\n",
    "\n",
    "# Use real config dimensions instead of dummy small sizes\n",
    "test_pred = torch.randn(batch_size, 1, image_size, image_size).to(device)\n",
    "test_target = torch.randint(0, 2, (batch_size, 1, image_size, image_size)).float().to(device)\n",
    "\n",
    "print(f\"Prediction shape: {test_pred.shape}\")\n",
    "print(f\"Target shape: {test_target.shape}\")\n",
    "\n",
    "# Test individual losses\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "dice_loss = DiceLoss()\n",
    "combined_loss = CombinedLoss()\n",
    "\n",
    "bce_val = bce_loss(test_pred, test_target)\n",
    "dice_val = dice_loss(test_pred, test_target)\n",
    "combined_val = combined_loss(test_pred, test_target)\n",
    "\n",
    "print(f\"BCE Loss: {bce_val.item():.4f}\")\n",
    "print(f\"Dice Loss: {dice_val.item():.4f}\")\n",
    "print(f\"Combined Loss: {combined_val.item():.4f}\")\n",
    "\n",
    "print(\"âœ“ Loss functions test successful!\")\n",
    "\n",
    "# Cleanup\n",
    "del test_pred, test_target, bce_val, dice_val, combined_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration Export\n",
    "Save model configuration for training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration updated and saved successfully\n",
      "Ready for training notebook!\n",
      "\n",
      "==================================================\n",
      "MODEL DEVELOPMENT COMPLETE\n",
      "==================================================\n",
      "âœ“ U-Net architecture implemented\n",
      "âœ“ Binary Cross Entropy + Dice Loss ready\n",
      "âœ“ Model tested successfully\n",
      "âœ“ Configuration saved for training\n",
      "âœ“ Ready for real data training\n",
      "\n",
      "Next: Run training & evaluation notebook\n"
     ]
    }
   ],
   "source": [
    "# Update configuration with model settings\n",
    "model_config = {\n",
    "    'model_name': 'UNet',\n",
    "    'n_channels': 3,\n",
    "    'n_classes': 1,\n",
    "    'total_parameters': total_params,\n",
    "    'trainable_parameters': trainable_params,\n",
    "    'loss_function': 'BCE_Dice_Combined',\n",
    "    'bce_weight': 0.5,\n",
    "    'dice_weight': 0.5,\n",
    "    'device': str(device)\n",
    "}\n",
    "\n",
    "# Merge with existing config\n",
    "config.update(model_config)\n",
    "\n",
    "# Save to config folder with proper naming\n",
    "os.makedirs('config', exist_ok=True)\n",
    "with open('config/model_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Configuration updated and saved successfully\")\n",
    "print(\"Ready for training notebook!\")\n",
    "\n",
    "# Final cleanup and summary\n",
    "gc.collect()\n",
    "print(\"MODEL DEVELOPMENT COMPLETE\")\n",
    "print(\"âœ“ U-Net architecture implemented\")\n",
    "print(\"âœ“ Binary Cross Entropy + Dice Loss ready\")\n",
    "print(\"âœ“ Model tested successfully\")\n",
    "print(\"âœ“ Configuration saved for training\")\n",
    "print(\"âœ“ Ready for real data training\")\n",
    "print(\"\\nNext: Run training & evaluation notebook\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oilspill-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
