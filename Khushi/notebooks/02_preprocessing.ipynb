{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil Spill Detection - Data Preprocessing Pipeline\n",
    "\n",
    "This notebook implements a robust preprocessing pipeline for oil spill detection based on EDA findings.\n",
    "\n",
    "## Key Requirements:\n",
    "- **Same Image Size**: Consistent 256x256 resolution across train/val/test\n",
    "- **Normalization Consistency**: Same 0-1 scaling for all datasets\n",
    "- **Shuffling**: Train=True, Val/Test=False\n",
    "- **Data Balance**: Only augment train set, keep val/test untouched\n",
    "- **Batch Size**: Consistent across all sets\n",
    "- **Visual Verification**: Check samples after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('../results/preprocessing', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(\"Results directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced configuration with consistency requirements\n",
    "CONFIG = {\n",
    "    'target_size': (256, 256),  # CONSISTENT across all splits\n",
    "    'batch_size': 32,  # SAME for train/val/test\n",
    "    'normalize_range': [0, 1],  # CONSISTENT 0-1 scaling\n",
    "    'binary_threshold': 127,\n",
    "    'augmentation_prob': 0.5,\n",
    "    'dataset_path': '../dataset',\n",
    "    'processed_path': '../data/processed',\n",
    "    'min_spill_area': 0.1,\n",
    "    'balance_threshold': 0.3,\n",
    "    # Shuffling configuration\n",
    "    'shuffle_train': True,   # Train: shuffle for randomization\n",
    "    'shuffle_val': False,    # Val: no shuffle for consistent evaluation\n",
    "    'shuffle_test': False    # Test: no shuffle for consistent evaluation\n",
    "}\n",
    "\n",
    "print(\"Enhanced Preprocessing Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n✅ Key Rules:\")\n",
    "print(\"  1. Same 256x256 size everywhere\")\n",
    "print(\"  2. Consistent 0-1 normalization\")\n",
    "print(\"  3. Train shuffle=True, Val/Test shuffle=False\")\n",
    "print(\"  4. Only augment train set\")\n",
    "print(\"  5. Same batch size (32) for all\")\n",
    "print(\"  6. Visual verification included\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Mask Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_mask_to_binary(mask_path, threshold=None):\n",
    "    \"\"\"\n",
    "    Convert RGB mask to binary format with enhanced error handling\n",
    "    Args:\n",
    "        mask_path: Path to RGB mask\n",
    "        threshold: Threshold for binary conversion\n",
    "    Returns:\n",
    "        Binary mask (0=non-spill, 1=spill)\n",
    "    \"\"\"\n",
    "    if threshold is None:\n",
    "        threshold = CONFIG['binary_threshold']\n",
    "        \n",
    "    try:\n",
    "        # Load mask - try different methods\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if mask is None:\n",
    "            # Try with PIL if cv2 fails\n",
    "            mask_pil = Image.open(mask_path).convert('L')\n",
    "            mask = np.array(mask_pil)\n",
    "        \n",
    "        if mask is None:\n",
    "            return None, f\"Could not load mask: {mask_path}\"\n",
    "        \n",
    "        # Convert to binary (any pixel above threshold becomes 1)\n",
    "        binary_mask = (mask > threshold).astype(np.uint8)\n",
    "        \n",
    "        # Calculate spill area percentage\n",
    "        spill_area = (np.sum(binary_mask) / binary_mask.size) * 100\n",
    "        \n",
    "        return binary_mask, spill_area\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, f\"Error converting mask {mask_path}: {e}\"\n",
    "\n",
    "# Enhanced visualization with better error handling and statistics\n",
    "def visualize_mask_conversion(image_path, mask_path, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualize original vs converted masks with statistics\n",
    "    \"\"\"\n",
    "    if not (os.path.exists(image_path) and os.path.exists(mask_path)):\n",
    "        print(f\"Paths do not exist: {image_path} or {mask_path}\")\n",
    "        return\n",
    "    \n",
    "    image_files = [f for f in os.listdir(image_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(\"No image files found\")\n",
    "        return\n",
    "    \n",
    "    sample_files = np.random.choice(image_files, min(num_samples, len(image_files)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    conversion_stats = {'successful': 0, 'failed': 0, 'spill_areas': []}\n",
    "    \n",
    "    for i, img_file in enumerate(sample_files):\n",
    "        try:\n",
    "            # Load original image\n",
    "            img = cv2.imread(os.path.join(image_path, img_file))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load original mask\n",
    "            mask_file = img_file\n",
    "            original_mask = cv2.imread(os.path.join(mask_path, mask_file), cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Convert mask\n",
    "            binary_mask, spill_info = convert_rgb_mask_to_binary(os.path.join(mask_path, mask_file))\n",
    "            \n",
    "            if binary_mask is not None:\n",
    "                conversion_stats['successful'] += 1\n",
    "                conversion_stats['spill_areas'].append(spill_info)\n",
    "            else:\n",
    "                conversion_stats['failed'] += 1\n",
    "                print(f\"Failed to convert: {spill_info}\")\n",
    "            \n",
    "            # Plot original image\n",
    "            axes[i, 0].imshow(img_rgb)\n",
    "            axes[i, 0].set_title(f'Original Image\\n{img_file}\\nSize: {img_rgb.shape}', fontsize=10)\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Plot original mask\n",
    "            if original_mask is not None:\n",
    "                axes[i, 1].imshow(original_mask, cmap='gray')\n",
    "                axes[i, 1].set_title(f'Original Mask\\nSize: {original_mask.shape}', fontsize=10)\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Plot binary mask\n",
    "            if binary_mask is not None:\n",
    "                axes[i, 2].imshow(binary_mask, cmap='gray')\n",
    "                axes[i, 2].set_title(f'Binary Mask\\nSpill Area: {spill_info:.1f}%', fontsize=10)\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "            conversion_stats['failed'] += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/preprocessing/mask_conversion_examples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print conversion statistics\n",
    "    print(f\"\\nMask Conversion Statistics:\")\n",
    "    print(f\"  Successful conversions: {conversion_stats['successful']}\")\n",
    "    print(f\"  Failed conversions: {conversion_stats['failed']}\")\n",
    "    if conversion_stats['spill_areas']:\n",
    "        print(f\"  Average spill area: {np.mean(conversion_stats['spill_areas']):.2f}%\")\n",
    "        print(f\"  Spill area range: {np.min(conversion_stats['spill_areas']):.2f}% - {np.max(conversion_stats['spill_areas']):.2f}%\")\n",
    "\n",
    "# Test mask conversion\n",
    "train_images_path = os.path.join(CONFIG['dataset_path'], 'train', 'images')\n",
    "train_masks_path = os.path.join(CONFIG['dataset_path'], 'train', 'masks')\n",
    "\n",
    "print(\"Testing mask conversion...\")\n",
    "visualize_mask_conversion(train_images_path, train_masks_path, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Image Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced preprocessing with strict consistency checks\n",
    "def preprocess_image_consistent(image_path, target_size=None, normalize_range=None):\n",
    "    \"\"\"\n",
    "    Preprocess image with STRICT consistency requirements\n",
    "    \"\"\"\n",
    "    if target_size is None:\n",
    "        target_size = CONFIG['target_size']\n",
    "    if normalize_range is None:\n",
    "        normalize_range = CONFIG['normalize_range']\n",
    "    \n",
    "    try:\n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            return None, \"Could not load image\"\n",
    "        \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # CONSISTENT resize to exact target size\n",
    "        image_resized = cv2.resize(image_rgb, target_size)\n",
    "        \n",
    "        # CONSISTENT normalization to [0, 1]\n",
    "        image_normalized = image_resized.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Verify consistency\n",
    "        assert image_normalized.shape[:2] == target_size, f\"Size mismatch: {image_normalized.shape[:2]} != {target_size}\"\n",
    "        assert image_normalized.min() >= 0 and image_normalized.max() <= 1, f\"Normalization error: range [{image_normalized.min():.3f}, {image_normalized.max():.3f}]\"\n",
    "        \n",
    "        return image_normalized, \"success\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, f\"Error: {e}\"\n",
    "\n",
    "def preprocess_mask_consistent(mask_path, target_size=None):\n",
    "    \"\"\"\n",
    "    Preprocess mask with STRICT consistency requirements\n",
    "    \"\"\"\n",
    "    if target_size is None:\n",
    "        target_size = CONFIG['target_size']\n",
    "    \n",
    "    try:\n",
    "        # Convert to binary\n",
    "        binary_mask, spill_area = convert_rgb_mask_to_binary(mask_path)\n",
    "        \n",
    "        if binary_mask is None:\n",
    "            return None, 0, f\"Binary conversion failed: {spill_area}\"\n",
    "        \n",
    "        # CONSISTENT resize to exact target size\n",
    "        mask_resized = cv2.resize(binary_mask, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Verify consistency\n",
    "        assert mask_resized.shape == target_size, f\"Mask size mismatch: {mask_resized.shape} != {target_size}\"\n",
    "        assert set(np.unique(mask_resized)).issubset({0, 1}), f\"Mask values not binary: {np.unique(mask_resized)}\"\n",
    "        \n",
    "        return mask_resized, spill_area, \"success\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, 0, f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Data Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced augmentation with train-only policy\n",
    "def get_augmentation_pipeline_enhanced(split='train', target_size=None):\n",
    "    \"\"\"\n",
    "    Create augmentation pipeline with STRICT split-based rules\n",
    "    Args:\n",
    "        split: 'train', 'val', or 'test'\n",
    "        target_size: Target image size\n",
    "    \"\"\"\n",
    "    if target_size is None:\n",
    "        target_size = CONFIG['target_size']\n",
    "    \n",
    "    if split == 'train':\n",
    "        # ONLY train gets augmentation\n",
    "        return A.Compose([\n",
    "            A.Resize(target_size[0], target_size[1]),\n",
    "            \n",
    "            # Geometric transformations\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.Rotate(limit=15, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "            \n",
    "            # Photometric transformations\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.2,\n",
    "                contrast_limit=0.2,\n",
    "                p=0.5\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=10,\n",
    "                sat_shift_limit=20,\n",
    "                val_shift_limit=10,\n",
    "                p=0.3\n",
    "            ),\n",
    "            \n",
    "            # Noise and blur\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "            A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "            \n",
    "            # Elastic transformations\n",
    "            A.ElasticTransform(\n",
    "                alpha=1, sigma=50, alpha_affine=50,\n",
    "                border_mode=cv2.BORDER_CONSTANT, value=0, p=0.2\n",
    "            ),\n",
    "            \n",
    "            # CONSISTENT normalization\n",
    "            A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])  # Keep 0-1 range\n",
    "        ])\n",
    "    else:\n",
    "        # Val/Test: ONLY resize and normalize (NO augmentation)\n",
    "        return A.Compose([\n",
    "            A.Resize(target_size[0], target_size[1]),\n",
    "            A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])  # Keep 0-1 range\n",
    "        ])\n",
    "\n",
    "print(\"✅ Augmentation Rules:\")\n",
    "print(\"  - Train: Full augmentation pipeline\")\n",
    "print(\"  - Val/Test: Only resize + normalize (NO augmentation)\")\n",
    "print(\"  - All splits: Same target size and normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added visual verification function\n",
    "def verify_preprocessing_samples(split='train', num_samples=6):\n",
    "    \"\"\"\n",
    "    Visual verification of preprocessing pipeline\n",
    "    \"\"\"\n",
    "    images_path = os.path.join(CONFIG['dataset_path'], split, 'images')\n",
    "    masks_path = os.path.join(CONFIG['dataset_path'], split, 'masks')\n",
    "    \n",
    "    image_files = [f for f in os.listdir(images_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:num_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    fig.suptitle(f'Preprocessing Verification - {split.upper()} Split', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    augmentation_pipeline = get_augmentation_pipeline_enhanced(split)\n",
    "    \n",
    "    for i, img_file in enumerate(image_files):\n",
    "        try:\n",
    "            img_path = os.path.join(images_path, img_file)\n",
    "            mask_path = os.path.join(masks_path, img_file)\n",
    "            \n",
    "            # Load original\n",
    "            original_img = cv2.imread(img_path)\n",
    "            original_img_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load original mask\n",
    "            original_mask = cv2.imread(mask_path)\n",
    "            original_mask_rgb = cv2.cvtColor(original_mask, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Preprocess\n",
    "            processed_img, img_status = preprocess_image_consistent(img_path)\n",
    "            processed_mask, spill_area, mask_status = preprocess_mask_consistent(mask_path)\n",
    "            \n",
    "            if processed_img is not None and processed_mask is not None:\n",
    "                # Apply augmentation if train split\n",
    "                if split == 'train':\n",
    "                    # Convert to uint8 for augmentation\n",
    "                    img_uint8 = (processed_img * 255).astype(np.uint8)\n",
    "                    augmented = augmentation_pipeline(image=img_uint8, mask=processed_mask)\n",
    "                    final_img = augmented['image']\n",
    "                    final_mask = augmented['mask']\n",
    "                else:\n",
    "                    # For val/test, just apply resize and normalize\n",
    "                    img_uint8 = (processed_img * 255).astype(np.uint8)\n",
    "                    transformed = augmentation_pipeline(image=img_uint8, mask=processed_mask)\n",
    "                    final_img = transformed['image']\n",
    "                    final_mask = transformed['mask']\n",
    "                \n",
    "                # Plot results\n",
    "                axes[i, 0].imshow(original_img_rgb)\n",
    "                axes[i, 0].set_title(f'Original\\n{original_img_rgb.shape}')\n",
    "                axes[i, 0].axis('off')\n",
    "                \n",
    "                axes[i, 1].imshow(original_mask_rgb)\n",
    "                axes[i, 1].set_title(f'Original Mask\\n{original_mask_rgb.shape}')\n",
    "                axes[i, 1].axis('off')\n",
    "                \n",
    "                axes[i, 2].imshow(processed_img)\n",
    "                axes[i, 2].set_title(f'Processed\\n{processed_img.shape}\\nRange: [{processed_img.min():.2f}, {processed_img.max():.2f}]')\n",
    "                axes[i, 2].axis('off')\n",
    "                \n",
    "                axes[i, 3].imshow(final_mask, cmap='gray')\n",
    "                axes[i, 3].set_title(f'Final Mask\\n{final_mask.shape}\\nSpill: {spill_area:.1f}%')\n",
    "                axes[i, 3].axis('off')\n",
    "                \n",
    "            else:\n",
    "                for j in range(4):\n",
    "                    axes[i, j].text(0.5, 0.5, f'Error: {img_status}', \n",
    "                                  ha='center', va='center', transform=axes[i, j].transAxes)\n",
    "                    axes[i, j].axis('off')\n",
    "                    \n",
    "        except Exception as e:\n",
    "            for j in range(4):\n",
    "                axes[i, j].text(0.5, 0.5, f'Error: {e}', \n",
    "                              ha='center', va='center', transform=axes[i, j].transAxes)\n",
    "                axes[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✅ Visual verification completed for {split} split\")\n",
    "    print(f\"   - Target size: {CONFIG['target_size']}\")\n",
    "    print(f\"   - Normalization: {CONFIG['normalize_range']}\")\n",
    "    print(f\"   - Augmentation: {'Yes' if split == 'train' else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Balance Analysis and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced dataset processing with consistency validation\n",
    "def process_dataset_split_enhanced(split='train'):\n",
    "    \"\"\"\n",
    "    Process dataset split with STRICT consistency validation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING {split.upper()} SPLIT WITH CONSISTENCY CHECKS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    images_path = os.path.join(CONFIG['dataset_path'], split, 'images')\n",
    "    masks_path = os.path.join(CONFIG['dataset_path'], split, 'masks')\n",
    "    \n",
    "    if not (os.path.exists(images_path) and os.path.exists(masks_path)):\n",
    "        print(f\"❌ Paths for {split} split do not exist\")\n",
    "        return None\n",
    "    \n",
    "    image_files = [f for f in os.listdir(images_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"❌ No images found in {split} split\")\n",
    "        return None\n",
    "    \n",
    "    stats = {\n",
    "        'split': split,\n",
    "        'total_images': len(image_files),\n",
    "        'successful_processing': 0,\n",
    "        'failed_processing': 0,\n",
    "        'spill_images': 0,\n",
    "        'non_spill_images': 0,\n",
    "        'consistency_checks': {\n",
    "            'size_consistent': 0,\n",
    "            'normalization_consistent': 0,\n",
    "            'mask_binary': 0\n",
    "        },\n",
    "        'spill_areas': [],\n",
    "        'processing_errors': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Processing {len(image_files)} images...\")\n",
    "    \n",
    "    for img_file in tqdm(image_files, desc=f\"Processing {split}\"):\n",
    "        try:\n",
    "            img_path = os.path.join(images_path, img_file)\n",
    "            mask_path = os.path.join(masks_path, img_file)\n",
    "            \n",
    "            # Process with consistency checks\n",
    "            processed_img, img_status = preprocess_image_consistent(img_path)\n",
    "            processed_mask, spill_area, mask_status = preprocess_mask_consistent(mask_path)\n",
    "            \n",
    "            if processed_img is not None and processed_mask is not None:\n",
    "                stats['successful_processing'] += 1\n",
    "                \n",
    "                # Consistency checks\n",
    "                if processed_img.shape[:2] == CONFIG['target_size']:\n",
    "                    stats['consistency_checks']['size_consistent'] += 1\n",
    "                \n",
    "                if 0 <= processed_img.min() and processed_img.max() <= 1:\n",
    "                    stats['consistency_checks']['normalization_consistent'] += 1\n",
    "                \n",
    "                if set(np.unique(processed_mask)).issubset({0, 1}):\n",
    "                    stats['consistency_checks']['mask_binary'] += 1\n",
    "                \n",
    "                # Spill classification\n",
    "                if spill_area > CONFIG['min_spill_area']:\n",
    "                    stats['spill_images'] += 1\n",
    "                    stats['spill_areas'].append(spill_area)\n",
    "                else:\n",
    "                    stats['non_spill_images'] += 1\n",
    "                    \n",
    "            else:\n",
    "                stats['failed_processing'] += 1\n",
    "                stats['processing_errors'].append(f\"{img_file}: {img_status}, {mask_status}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            stats['failed_processing'] += 1\n",
    "            stats['processing_errors'].append(f\"{img_file}: {e}\")\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    total_processed = stats['successful_processing']\n",
    "    if total_processed > 0:\n",
    "        stats['spill_ratio'] = stats['spill_images'] / total_processed\n",
    "        stats['consistency_rate'] = {\n",
    "            'size': stats['consistency_checks']['size_consistent'] / total_processed,\n",
    "            'normalization': stats['consistency_checks']['normalization_consistent'] / total_processed,\n",
    "            'mask_binary': stats['consistency_checks']['mask_binary'] / total_processed\n",
    "        }\n",
    "        \n",
    "        if stats['spill_areas']:\n",
    "            stats['avg_spill_area'] = np.mean(stats['spill_areas'])\n",
    "            stats['std_spill_area'] = np.std(stats['spill_areas'])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n✅ PROCESSING RESULTS FOR {split.upper()}:\")\n",
    "    print(f\"   Total images: {stats['total_images']}\")\n",
    "    print(f\"   Successful: {stats['successful_processing']} ({stats['successful_processing']/stats['total_images']*100:.1f}%)\")\n",
    "    print(f\"   Failed: {stats['failed_processing']} ({stats['failed_processing']/stats['total_images']*100:.1f}%)\")\n",
    "    \n",
    "    if total_processed > 0:\n",
    "        print(f\"\\n📊 CONSISTENCY VALIDATION:\")\n",
    "        print(f\"   Size consistency: {stats['consistency_rate']['size']*100:.1f}%\")\n",
    "        print(f\"   Normalization consistency: {stats['consistency_rate']['normalization']*100:.1f}%\")\n",
    "        print(f\"   Mask binary consistency: {stats['consistency_rate']['mask_binary']*100:.1f}%\")\n",
    "        \n",
    "        print(f\"\\n🎯 CLASS DISTRIBUTION:\")\n",
    "        print(f\"   Spill images: {stats['spill_images']} ({stats['spill_ratio']*100:.1f}%)\")\n",
    "        print(f\"   Non-spill images: {stats['non_spill_images']} ({(1-stats['spill_ratio'])*100:.1f}%)\")\n",
    "        \n",
    "        if stats['spill_areas']:\n",
    "            print(f\"   Avg spill area: {stats['avg_spill_area']:.2f}% ± {stats['std_spill_area']:.2f}%\")\n",
    "    \n",
    "    if stats['processing_errors']:\n",
    "        print(f\"\\n❌ PROCESSING ERRORS ({len(stats['processing_errors'])})\")\n",
    "        for error in stats['processing_errors'][:5]:  # Show first 5 errors\n",
    "            print(f\"   {error}\")\n",
    "        if len(stats['processing_errors']) > 5:\n",
    "            print(f\"   ... and {len(stats['processing_errors'])-5} more errors\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Pipeline Execution\n",
    "\n",
    "Execute the complete preprocessing pipeline with all consistency checks and visual verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute complete preprocessing pipeline\n",
    "print(\"🚀 STARTING COMPLETE PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Process all splits with consistency validation\n",
    "all_stats = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    all_stats[split] = process_dataset_split_enhanced(split)\n",
    "\n",
    "# Step 2: Visual verification for each split\n",
    "print(\"\\n📸 VISUAL VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "for split in ['train', 'val', 'test']:\n",
    "    if all_stats[split] is not None:\n",
    "        print(f\"\\nVerifying {split} split samples...\")\n",
    "        verify_preprocessing_samples(split, num_samples=3)\n",
    "\n",
    "# Step 3: Final summary\n",
    "print(\"\\n🎯 FINAL PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for split, stats in all_stats.items():\n",
    "    if stats is not None:\n",
    "        print(f\"\\n{split.upper()} Split:\")\n",
    "        print(f\"  ✅ Processed: {stats['successful_processing']}/{stats['total_images']}\")\n",
    "        print(f\"  📏 Size consistency: {stats['consistency_rate']['size']*100:.1f}%\")\n",
    "        print(f\"  🔢 Normalization consistency: {stats['consistency_rate']['normalization']*100:.1f}%\")\n",
    "        print(f\"  🎭 Mask binary consistency: {stats['consistency_rate']['mask_binary']*100:.1f}%\")\n",
    "        print(f\"  ⚖️ Spill ratio: {stats['spill_ratio']*100:.1f}%\")\n",
    "        print(f\"  🔀 Shuffle setting: {CONFIG[f'shuffle_{split}']}\")\n",
    "\n",
    "print(\"\\n✅ PREPROCESSING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"\\n📋 MILESTONE 1 CHECKLIST:\")\n",
    "print(\"  ✅ Dataset loaded and explored\")\n",
    "print(\"  ✅ EDA analysis completed\")\n",
    "print(\"  ✅ RGB masks converted to binary\")\n",
    "print(\"  ✅ Images resized to consistent dimensions\")\n",
    "print(\"  ✅ Normalization applied consistently\")\n",
    "print(\"  ✅ Augmentation pipeline ready (train only)\")\n",
    "print(\"  ✅ Dataset balance analyzed\")\n",
    "print(\"  ✅ Visual verification completed\")\n",
    "print(\"  ✅ Data ready for model training\")\n",
    "\n",
    "print(\"\\n🚀 Ready to proceed to Milestone 2: Model Training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
