{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil Spill Detection - Data Preprocessing Pipeline\n",
    "\n",
    "This notebook implements a robust preprocessing pipeline for oil spill detection based on EDA findings.\n",
    "\n",
    "## Key Requirements:\n",
    "- **Same Image Size**: Consistent 256x256 resolution across train/val/test\n",
    "- **Normalization Consistency**: Same 0-1 scaling for all datasets\n",
    "- **Shuffling**: Train=True, Val/Test=False\n",
    "- **Data Balance**: Only augment train set, keep val/test untouched\n",
    "- **Batch Size**: Consistent across all sets\n",
    "- **Visual Verification**: Check samples after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('../results/preprocessing', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(\"Results directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced configuration with consistency requirements\n",
    "CONFIG = {\n",
    "    'target_size': (256, 256),  # CONSISTENT across all splits\n",
    "    'batch_size': 32,  # SAME for train/val/test\n",
    "    'normalize_range': [0, 1],  # CONSISTENT 0-1 scaling\n",
    "    'binary_threshold': 127,\n",
    "    'augmentation_prob': 0.5,\n",
    "    'dataset_path': '../dataset',\n",
    "    'processed_path': '../data/processed',\n",
    "    'min_spill_area': 0.1,\n",
    "    'balance_threshold': 0.3,\n",
    "    # Shuffling configuration\n",
    "    'shuffle_train': True,   # Train: shuffle for randomization\n",
    "    'shuffle_val': False,    # Val: no shuffle for consistent evaluation\n",
    "    'shuffle_test': False    # Test: no shuffle for consistent evaluation\n",
    "}\n",
    "\n",
    "print(\"Enhanced Preprocessing Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nâœ… Key Rules:\")\n",
    "print(\"  1. Same 256x256 size everywhere\")\n",
    "print(\"  2. Consistent 0-1 normalization\")\n",
    "print(\"  3. Train shuffle=True, Val/Test shuffle=False\")\n",
    "print(\"  4. Only augment train set\")\n",
    "print(\"  5. Same batch size (32) for all\")\n",
    "print(\"  6. Visual verification included\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Mask Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_mask_to_binary(mask_path, threshold=None):\n",
    "    \"\"\"\n",
    "    Convert RGB mask to binary format with enhanced error handling\n",
    "    Args:\n",
    "        mask_path: Path to RGB mask\n",
    "        threshold: Threshold for binary conversion\n",
    "    Returns:\n",
    "        Binary mask (0=non-spill, 1=spill)\n",
    "    \"\"\"\n",
    "    if threshold is None:\n",
    "        threshold = CONFIG['binary_threshold']\n",
    "        \n",
    "    try:\n",
    "        # Load mask - try different methods\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if mask is None:\n",
    "            # Try with PIL if cv2 fails\n",
    "            mask_pil = Image.open(mask_path).convert('L')\n",
    "            mask = np.array(mask_pil)\n",
    "        \n",
    "        if mask is None:\n",
    "            return None, f\"Could not load mask: {mask_path}\"\n",
    "        \n",
    "        # Convert to binary (any pixel above threshold becomes 1)\n",
    "        binary_mask = (mask > threshold).astype(np.uint8)\n",
    "        \n",
    "        # Calculate spill area percentage\n",
    "        spill_area = (np.sum(binary_mask) / binary_mask.size) * 100\n",
    "        \n",
    "        return binary_mask, spill_area\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, f\"Error converting mask {mask_path}: {e}\"\n",
    "\n",
    "# Enhanced visualization with better error handling and statistics\n",
    "def visualize_mask_conversion(image_path, mask_path, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualize original vs converted masks with statistics\n",
    "    \"\"\"\n",
    "    if not (os.path.exists(image_path) and os.path.exists(mask_path)):\n",
    "        print(f\"Paths do not exist: {image_path} or {mask_path}\")\n",
    "        return\n",
    "    \n",
    "    image_files = [f for f in os.listdir(image_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(\"No image files found\")\n",
    "        return\n",
    "    \n",
    "    sample_files = np.random.choice(image_files, min(num_samples, len(image_files)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    conversion_stats = {'successful': 0, 'failed': 0, 'spill_areas': []}\n",
    "    \n",
    "    for i, img_file in enumerate(sample_files):\n",
    "        try:\n",
    "            # Load original image\n",
    "            img = cv2.imread(os.path.join(image_path, img_file))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load original mask\n",
    "            mask_file = img_file\n",
    "            original_mask = cv2.imread(os.path.join(mask_path, mask_file), cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Convert mask\n",
    "            binary_mask, spill_info = convert_rgb_mask_to_binary(os.path.join(mask_path, mask_file))\n",
    "            \n",
    "            if binary_mask is not None:\n",
    "                conversion_stats['successful'] += 1\n",
    "                conversion_stats['spill_areas'].append(spill_info)\n",
    "            else:\n",
    "                conversion_stats['failed'] += 1\n",
    "                print(f\"Failed to convert: {spill_info}\")\n",
    "            \n",
    "            # Plot original image\n",
    "            axes[i, 0].imshow(img_rgb)\n",
    "            axes[i, 0].set_title(f'Original Image\\n{img_file}\\nSize: {img_rgb.shape}', fontsize=10)\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Plot original mask\n",
    "            if original_mask is not None:\n",
    "                axes[i, 1].imshow(original_mask, cmap='gray')\n",
    "                axes[i, 1].set_title(f'Original Mask\\nSize: {original_mask.shape}', fontsize=10)\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Plot binary mask\n",
    "            if binary_mask is not None:\n",
    "                axes[i, 2].imshow(binary_mask, cmap='gray')\n",
    "                axes[i, 2].set_title(f'Binary Mask\\nSpill Area: {spill_info:.1f}%', fontsize=10)\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "            conversion_stats['failed'] += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/preprocessing/mask_conversion_examples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print conversion statistics\n",
    "    print(f\"\\nMask Conversion Statistics:\")\n",
    "    print(f\"  Successful conversions: {conversion_stats['successful']}\")\n",
    "    print(f\"  Failed conversions: {conversion_stats['failed']}\")\n",
    "    if conversion_stats['spill_areas']:\n",
    "        print(f\"  Average spill area: {np.mean(conversion_stats['spill_areas']):.2f}%\")\n",
    "        print(f\"  Spill area range: {np.min(conversion_stats['spill_areas']):.2f}% - {np.max(conversion_stats['spill_areas']):.2f}%\")\n",
    "\n",
    "# Test mask conversion\n",
    "train_images_path = os.path.join(CONFIG['dataset_path'], 'train', 'images')\n",
    "train_masks_path = os.path.join(CONFIG['dataset_path'], 'train', 'masks')\n",
    "\n",
    "print(\"Testing mask conversion...\")\n",
    "visualize_mask_conversion(train_images_path, train_masks_path, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Image Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced preprocessing with strict consistency checks\n",
    "def preprocess_image_consistent(image_path, target_size=None, normalize_range=None):\n",
    "    \"\"\"\n",
    "    Preprocess image with STRICT consistency requirements\n",
    "    \"\"\"\n",
    "    if target_size is None:\n",
    "        target_size = CONFIG['target_size']\n",
    "    if normalize_range is None:\n",
    "        normalize_range = CONFIG['normalize_range']\n",
    "    \n",
    "    try:\n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            return None, \"Could not load image\"\n",
    "        \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # CONSISTENT resize to exact target size\n",
    "        image_resized = cv2.resize(image_rgb, target_size)\n",
    "        \n",
    "        # CONSISTENT normalization to [0, 1]\n",
    "        image_normalized = image_resized.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Verify consistency\n",
    "        assert image_normalized.shape[:2] == target_size, f\"Size mismatch: {image_normalized.shape[:2]} != {target_size}\"\n",
    "        assert image_normalized.min() >= 0 and image_normalized.max() <= 1, f\"Normalization error: range [{image_normalized.min():.3f}, {image_normalized.max():.3f}]\"\n",
    "        \n",
    "        return image_normalized, \"success\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, f\"Error: {e}\"\n",
    "\n",
    "def preprocess_mask_consistent(mask_path, target_size=None):\n",
    "    \"\"\"\n",
    "    Preprocess mask with STRICT consistency requirements\n",
    "    \"\"\"\n",
    "    if target_size is None:\n",
    "        target_size = CONFIG['target_size']\n",
    "    \n",
    "    try:\n",
    "        # Convert to binary\n",
    "        binary_mask, spill_area = convert_rgb_mask_to_binary(mask_path)\n",
    "        \n",
    "        if binary_mask is None:\n",
    "            return None, 0, f\"Binary conversion failed: {spill_area}\"\n",
    "        \n",
    "        # CONSISTENT resize to exact target size\n",
    "        mask_resized = cv2.resize(binary_mask, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Verify consistency\n",
    "        assert mask_resized.shape == target_size, f\"Mask size mismatch: {mask_resized.shape} != {target_size}\"\n",
    "        assert set(np.unique(mask_resized)).issubset({0, 1}), f\"Mask values not binary: {np.unique(mask_resized)}\"\n",
    "        \n",
    "        return mask_resized, spill_area, \"success\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, 0, f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Data Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced augmentation with train-only policy\n",
    "def get_augmentation_pipeline_enhanced(split='train', target_size=None):\n",
    "    \"\"\"\n",
    "    Create augmentation pipeline with STRICT split-based rules\n",
    "    Args:\n",
    "        split: 'train', 'val', or 'test'\n",
    "        target_size: Target image size\n",
    "    \"\"\"\n",
    "    if target_size is None:\n",
    "        target_size = CONFIG['target_size']\n",
    "    \n",
    "    if split == 'train':\n",
    "        # ONLY train gets augmentation\n",
    "        return A.Compose([\n",
    "            A.Resize(target_size[0], target_size[1]),\n",
    "            \n",
    "            # Geometric transformations\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.Rotate(limit=15, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "            \n",
    "            # Photometric transformations\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.2,\n",
    "                contrast_limit=0.2,\n",
    "                p=0.5\n",
    "            ),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=10,\n",
    "                sat_shift_limit=20,\n",
    "                val_shift_limit=10,\n",
    "                p=0.3\n",
    "            ),\n",
    "            \n",
    "            # Noise and blur\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "            A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "            \n",
    "            # Elastic transformations\n",
    "            A.ElasticTransform(\n",
    "                alpha=1, sigma=50, alpha_affine=50,\n",
    "                border_mode=cv2.BORDER_CONSTANT, value=0, p=0.2\n",
    "            ),\n",
    "            \n",
    "            # CONSISTENT normalization\n",
    "            A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])  # Keep 0-1 range\n",
    "        ])\n",
    "    else:\n",
    "        # Val/Test: ONLY resize and normalize (NO augmentation)\n",
    "        return A.Compose([\n",
    "            A.Resize(target_size[0], target_size[1]),\n",
    "            A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])  # Keep 0-1 range\n",
    "        ])\n",
    "\n",
    "print(\"âœ… Augmentation Rules:\")\n",
    "print(\"  - Train: Full augmentation pipeline\")\n",
    "print(\"  - Val/Test: Only resize + normalize (NO augmentation)\")\n",
    "print(\"  - All splits: Same target size and normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added visual verification function\n",
    "def verify_preprocessing_samples(split='train', num_samples=6):\n",
    "    \"\"\"\n",
    "    Visual verification of preprocessing pipeline\n",
    "    \"\"\"\n",
    "    images_path = os.path.join(CONFIG['dataset_path'], split, 'images')\n",
    "    masks_path = os.path.join(CONFIG['dataset_path'], split, 'masks')\n",
    "    \n",
    "    image_files = [f for f in os.listdir(images_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:num_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    fig.suptitle(f'Preprocessing Verification - {split.upper()} Split', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    augmentation_pipeline = get_augmentation_pipeline_enhanced(split)\n",
    "    \n",
    "    for i, img_file in enumerate(image_files):\n",
    "        try:\n",
    "            img_path = os.path.join(images_path, img_file)\n",
    "            mask_path = os.path.join(masks_path, img_file)\n",
    "            \n",
    "            # Load original\n",
    "            original_img = cv2.imread(img_path)\n",
    "            original_img_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Load original mask\n",
    "            original_mask = cv2.imread(mask_path)\n",
    "            original_mask_rgb = cv2.cvtColor(original_mask, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Preprocess\n",
    "            processed_img, img_status = preprocess_image_consistent(img_path)\n",
    "            processed_mask, spill_area, mask_status = preprocess_mask_consistent(mask_path)\n",
    "            \n",
    "            if processed_img is not None and processed_mask is not None:\n",
    "                # Apply augmentation if train split\n",
    "                if split == 'train':\n",
    "                    # Convert to uint8 for augmentation\n",
    "                    img_uint8 = (processed_img * 255).astype(np.uint8)\n",
    "                    augmented = augmentation_pipeline(image=img_uint8, mask=processed_mask)\n",
    "                    final_img = augmented['image']\n",
    "                    final_mask = augmented['mask']\n",
    "                else:\n",
    "                    # For val/test, just apply resize and normalize\n",
    "                    img_uint8 = (processed_img * 255).astype(np.uint8)\n",
    "                    transformed = augmentation_pipeline(image=img_uint8, mask=processed_mask)\n",
    "                    final_img = transformed['image']\n",
    "                    final_mask = transformed['mask']\n",
    "                \n",
    "                # Plot results\n",
    "                axes[i, 0].imshow(original_img_rgb)\n",
    "                axes[i, 0].set_title(f'Original\\n{original_img_rgb.shape}')\n",
    "                axes[i, 0].axis('off')\n",
    "                \n",
    "                axes[i, 1].imshow(original_mask_rgb)\n",
    "                axes[i, 1].set_title(f'Original Mask\\n{original_mask_rgb.shape}')\n",
    "                axes[i, 1].axis('off')\n",
    "                \n",
    "                axes[i, 2].imshow(processed_img)\n",
    "                axes[i, 2].set_title(f'Processed\\n{processed_img.shape}\\nRange: [{processed_img.min():.2f}, {processed_img.max():.2f}]')\n",
    "                axes[i, 2].axis('off')\n",
    "                \n",
    "                axes[i, 3].imshow(final_mask, cmap='gray')\n",
    "                axes[i, 3].set_title(f'Final Mask\\n{final_mask.shape}\\nSpill: {spill_area:.1f}%')\n",
    "                axes[i, 3].axis('off')\n",
    "                \n",
    "            else:\n",
    "                for j in range(4):\n",
    "                    axes[i, j].text(0.5, 0.5, f'Error: {img_status}', \n",
    "                                  ha='center', va='center', transform=axes[i, j].transAxes)\n",
    "                    axes[i, j].axis('off')\n",
    "                    \n",
    "        except Exception as e:\n",
    "            for j in range(4):\n",
    "                axes[i, j].text(0.5, 0.5, f'Error: {e}', \n",
    "                              ha='center', va='center', transform=axes[i, j].transAxes)\n",
    "                axes[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ… Visual verification completed for {split} split\")\n",
    "    print(f\"   - Target size: {CONFIG['target_size']}\")\n",
    "    print(f\"   - Normalization: {CONFIG['normalize_range']}\")\n",
    "    print(f\"   - Augmentation: {'Yes' if split == 'train' else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Balance Analysis and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced dataset processing with consistency validation\n",
    "def process_dataset_split_enhanced(split='train'):\n",
    "    \"\"\"\n",
    "    Process dataset split with STRICT consistency validation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING {split.upper()} SPLIT WITH CONSISTENCY CHECKS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    images_path = os.path.join(CONFIG['dataset_path'], split, 'images')\n",
    "    masks_path = os.path.join(CONFIG['dataset_path'], split, 'masks')\n",
    "    \n",
    "    if not (os.path.exists(images_path) and os.path.exists(masks_path)):\n",
    "        print(f\"âŒ Paths for {split} split do not exist\")\n",
    "        return None\n",
    "    \n",
    "    image_files = [f for f in os.listdir(images_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"âŒ No images found in {split} split\")\n",
    "        return None\n",
    "    \n",
    "    stats = {\n",
    "        'split': split,\n",
    "        'total_images': len(image_files),\n",
    "        'successful_processing': 0,\n",
    "        'failed_processing': 0,\n",
    "        'spill_images': 0,\n",
    "        'non_spill_images': 0,\n",
    "        'consistency_checks': {\n",
    "            'size_consistent': 0,\n",
    "            'normalization_consistent': 0,\n",
    "            'mask_binary': 0\n",
    "        },\n",
    "        'spill_areas': [],\n",
    "        'processing_errors': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Processing {len(image_files)} images...\")\n",
    "    \n",
    "    for img_file in tqdm(image_files, desc=f\"Processing {split}\"):\n",
    "        try:\n",
    "            img_path = os.path.join(images_path, img_file)\n",
    "            mask_path = os.path.join(masks_path, img_file)\n",
    "            \n",
    "            # Process with consistency checks\n",
    "            processed_img, img_status = preprocess_image_consistent(img_path)\n",
    "            processed_mask, spill_area, mask_status = preprocess_mask_consistent(mask_path)\n",
    "            \n",
    "            if processed_img is not None and processed_mask is not None:\n",
    "                stats['successful_processing'] += 1\n",
    "                \n",
    "                # Consistency checks\n",
    "                if processed_img.shape[:2] == CONFIG['target_size']:\n",
    "                    stats['consistency_checks']['size_consistent'] += 1\n",
    "                \n",
    "                if 0 <= processed_img.min() and processed_img.max() <= 1:\n",
    "                    stats['consistency_checks']['normalization_consistent'] += 1\n",
    "                \n",
    "                if set(np.unique(processed_mask)).issubset({0, 1}):\n",
    "                    stats['consistency_checks']['mask_binary'] += 1\n",
    "                \n",
    "                # Spill classification\n",
    "                if spill_area > CONFIG['min_spill_area']:\n",
    "                    stats['spill_images'] += 1\n",
    "                    stats['spill_areas'].append(spill_area)\n",
    "                else:\n",
    "                    stats['non_spill_images'] += 1\n",
    "                    \n",
    "            else:\n",
    "                stats['failed_processing'] += 1\n",
    "                stats['processing_errors'].append(f\"{img_file}: {img_status}, {mask_status}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            stats['failed_processing'] += 1\n",
    "            stats['processing_errors'].append(f\"{img_file}: {e}\")\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    total_processed = stats['successful_processing']\n",
    "    if total_processed > 0:\n",
    "        stats['spill_ratio'] = stats['spill_images'] / total_processed\n",
    "        stats['consistency_rate'] = {\n",
    "            'size': stats['consistency_checks']['size_consistent'] / total_processed,\n",
    "            'normalization': stats['consistency_checks']['normalization_consistent'] / total_processed,\n",
    "            'mask_binary': stats['consistency_checks']['mask_binary'] / total_processed\n",
    "        }\n",
    "        \n",
    "        if stats['spill_areas']:\n",
    "            stats['avg_spill_area'] = np.mean(stats['spill_areas'])\n",
    "            stats['std_spill_area'] = np.std(stats['spill_areas'])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nâœ… PROCESSING RESULTS FOR {split.upper()}:\")\n",
    "    print(f\"   Total images: {stats['total_images']}\")\n",
    "    print(f\"   Successful: {stats['successful_processing']} ({stats['successful_processing']/stats['total_images']*100:.1f}%)\")\n",
    "    print(f\"   Failed: {stats['failed_processing']} ({stats['failed_processing']/stats['total_images']*100:.1f}%)\")\n",
    "    \n",
    "    if total_processed > 0:\n",
    "        print(f\"\\nğŸ“Š CONSISTENCY VALIDATION:\")\n",
    "        print(f\"   Size consistency: {stats['consistency_rate']['size']*100:.1f}%\")\n",
    "        print(f\"   Normalization consistency: {stats['consistency_rate']['normalization']*100:.1f}%\")\n",
    "        print(f\"   Mask binary consistency: {stats['consistency_rate']['mask_binary']*100:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ CLASS DISTRIBUTION:\")\n",
    "        print(f\"   Spill images: {stats['spill_images']} ({stats['spill_ratio']*100:.1f}%)\")\n",
    "        print(f\"   Non-spill images: {stats['non_spill_images']} ({(1-stats['spill_ratio'])*100:.1f}%)\")\n",
    "        \n",
    "        if stats['spill_areas']:\n",
    "            print(f\"   Avg spill area: {stats['avg_spill_area']:.2f}% Â± {stats['std_spill_area']:.2f}%\")\n",
    "    \n",
    "    if stats['processing_errors']:\n",
    "        print(f\"\\nâŒ PROCESSING ERRORS ({len(stats['processing_errors'])})\")\n",
    "        for error in stats['processing_errors'][:5]:  # Show first 5 errors\n",
    "            print(f\"   {error}\")\n",
    "        if len(stats['processing_errors']) > 5:\n",
    "            print(f\"   ... and {len(stats['processing_errors'])-5} more errors\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Pipeline Execution\n",
    "\n",
    "Execute the complete preprocessing pipeline with all consistency checks and visual verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute complete preprocessing pipeline\n",
    "print(\"ğŸš€ STARTING COMPLETE PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Process all splits with consistency validation\n",
    "all_stats = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    all_stats[split] = process_dataset_split_enhanced(split)\n",
    "\n",
    "# Step 2: Visual verification for each split\n",
    "print(\"\\nğŸ“¸ VISUAL VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "for split in ['train', 'val', 'test']:\n",
    "    if all_stats[split] is not None:\n",
    "        print(f\"\\nVerifying {split} split samples...\")\n",
    "        verify_preprocessing_samples(split, num_samples=3)\n",
    "\n",
    "# Step 3: Final summary\n",
    "print(\"\\nğŸ¯ FINAL PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for split, stats in all_stats.items():\n",
    "    if stats is not None:\n",
    "        print(f\"\\n{split.upper()} Split:\")\n",
    "        print(f\"  âœ… Processed: {stats['successful_processing']}/{stats['total_images']}\")\n",
    "        print(f\"  ğŸ“ Size consistency: {stats['consistency_rate']['size']*100:.1f}%\")\n",
    "        print(f\"  ğŸ”¢ Normalization consistency: {stats['consistency_rate']['normalization']*100:.1f}%\")\n",
    "        print(f\"  ğŸ­ Mask binary consistency: {stats['consistency_rate']['mask_binary']*100:.1f}%\")\n",
    "        print(f\"  âš–ï¸ Spill ratio: {stats['spill_ratio']*100:.1f}%\")\n",
    "        print(f\"  ğŸ”€ Shuffle setting: {CONFIG[f'shuffle_{split}']}\")\n",
    "\n",
    "print(\"\\nâœ… PREPROCESSING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"\\nğŸ“‹ MILESTONE 1 CHECKLIST:\")\n",
    "print(\"  âœ… Dataset loaded and explored\")\n",
    "print(\"  âœ… EDA analysis completed\")\n",
    "print(\"  âœ… RGB masks converted to binary\")\n",
    "print(\"  âœ… Images resized to consistent dimensions\")\n",
    "print(\"  âœ… Normalization applied consistently\")\n",
    "print(\"  âœ… Augmentation pipeline ready (train only)\")\n",
    "print(\"  âœ… Dataset balance analyzed\")\n",
    "print(\"  âœ… Visual verification completed\")\n",
    "print(\"  âœ… Data ready for model training\")\n",
    "\n",
    "print(\"\\nğŸš€ Ready to proceed to Milestone 2: Model Training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
