{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil Spill Segmentation - Training & Evaluation\n",
    "\n",
    "**Train and evaluate U-Net for oil spill segmentation, with early stopping based on validation IoU/F1.**\n",
    "\n",
    "- Training stops when validation mIoU stops improving for several epochs (patience).\n",
    "- All key metrics shown (IoU, Dice, F1, accuracy, precision, recall).\n",
    "- False alarm (FP) rates: aim for F1 > 0.7 and IoU > 0.6, false alarms 5-10% acceptable.\n",
    "- Continue training >20 epochs if metrics keep improving.\n",
    "- Save best model by validation mIoU.\n",
    "\n",
    "Run this after dataset and model notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports & Setup\n",
    "import os, random, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "# Load config from previous notebook\n",
    "with open('config/model_config.json', 'r') as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "# Make results dir\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device(cfg.get('device','cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print('Using device:', device)\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class (should match your previous dataset pipeline exactly)\n",
    "from torch.utils.data import Dataset\n",
    "class OilSpillDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.images = []\n",
    "        if os.path.exists(image_dir):\n",
    "            self.images = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "            self.images.sort()\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L') if os.path.exists(mask_path) else Image.new('L', image.size, 0)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "        mask = (mask > 0.5).float()\n",
    "        return image, mask.squeeze(0).long(), img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "from torchvision import transforms\n",
    "def get_transforms(is_training=True):\n",
    "    if is_training:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((cfg['image_size'], cfg['image_size'])),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((cfg['image_size'], cfg['image_size'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "        ])\n",
    "def get_mask_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((cfg['image_size'], cfg['image_size'])),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 811  Val samples: 203\n"
     ]
    }
   ],
   "source": [
    "# DataLoader setup\n",
    "train_dataset = OilSpillDataset(\n",
    "    image_dir=cfg['data_paths']['train_images'],\n",
    "    mask_dir=cfg['data_paths']['train_masks'],\n",
    "    transform=get_transforms(is_training=True),\n",
    "    mask_transform=get_mask_transforms()\n",
    ")\n",
    "val_dataset = OilSpillDataset(\n",
    "    image_dir=cfg['data_paths']['val_images'],\n",
    "    mask_dir=cfg['data_paths']['val_masks'],\n",
    "    transform=get_transforms(is_training=False),\n",
    "    mask_transform=get_mask_transforms()\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True, num_workers=cfg['num_workers'], pin_memory=cfg['pin_memory'])\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg['batch_size'], shuffle=False, num_workers=cfg['num_workers'], pin_memory=cfg['pin_memory'])\n",
    "print(f\"Train samples: {len(train_dataset)}  Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load U-Net model (from previous notebook)\n",
    "import torch.nn.functional as F\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, 2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super().__init__()\n",
    "        self.inc = DoubleConv(n_channels,64)\n",
    "        self.down1 = Down(64,128)\n",
    "        self.down2 = Down(128,256)\n",
    "        self.down3 = Down(256,512)\n",
    "        self.down4 = Down(512,1024)\n",
    "        self.up1 = Up(1024,512)\n",
    "        self.up2 = Up(512,256)\n",
    "        self.up3 = Up(256,128)\n",
    "        self.up4 = Up(128,64)\n",
    "        self.outc = nn.Conv2d(64, n_classes, 1)\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "# Instantiate model\n",
    "model = UNet(n_channels=3, n_classes=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define binary loss: BCE + Dice\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, preds, targets):\n",
    "        preds = torch.sigmoid(preds)\n",
    "        preds = preds.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (preds * targets).sum()\n",
    "        dice = (2.*intersection + self.smooth) / (preds.sum() + targets.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "    def forward(self, preds, targets):\n",
    "        return self.bce_weight*self.bce(preds, targets.float()) + self.dice_weight*self.dice(preds, targets.float())\n",
    "criterion = CombinedLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_46524\\3897414160.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
     ]
    }
   ],
   "source": [
    "# Optimizer & Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function (IoU, Dice, F1, accuracy, precision, recall)\n",
    "def calc_metrics(preds, targets):\n",
    "    preds = preds.cpu().numpy().astype(np.uint8)\n",
    "    targets = targets.cpu().numpy().astype(np.uint8)\n",
    "    acc = (preds == targets).mean()\n",
    "    tp = np.logical_and(preds==1, targets==1).sum()\n",
    "    fp = np.logical_and(preds==1, targets==0).sum()\n",
    "    fn = np.logical_and(preds==0, targets==1).sum()\n",
    "    precision = tp/(tp+fp+1e-8)\n",
    "    recall = tp/(tp+fn+1e-8)\n",
    "    f1 = 2*precision*recall/(precision+recall+1e-8)\n",
    "    inter = np.logical_and(preds==1, targets==1).sum()\n",
    "    union = np.logical_or(preds==1, targets==1).sum()\n",
    "    iou = inter/union if union>0 else 1.0\n",
    "    dice = (2*inter)/(preds.sum()+targets.sum()+1e-8) if (preds.sum()+targets.sum())>0 else 1.0\n",
    "    return {\n",
    "        'acc': float(acc), 'iou': float(iou), 'dice': float(dice),\n",
    "        'precision': float(precision), 'recall': float(recall), 'f1': float(f1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping by validation IoU\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=6):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "    def step(self, score):\n",
    "        if self.best_score is None or score > self.best_score:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            return True  # Improved\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            return False\n",
    "    def should_stop(self):\n",
    "        return self.counter >= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loop\n",
    "def run_epoch(loader, model, criterion, optimizer=None, scaler=None, train=False):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    tot_loss, tot_metrics, n = 0.0, {'acc':0,'iou':0,'dice':0,'precision':0,'recall':0,'f1':0}, 0\n",
    "    pbar = tqdm(loader, desc='Train' if train else 'Val', leave=False)\n",
    "    for img, mask, _ in pbar:\n",
    "        img, mask = img.to(device), mask.to(device)\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
    "                out = model(img)\n",
    "                loss = criterion(out, mask.unsqueeze(1))\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = model(img)\n",
    "                loss = criterion(out, mask.unsqueeze(1))\n",
    "        with torch.no_grad():\n",
    "            prob = torch.sigmoid(out)\n",
    "            pred = (prob > 0.5).long().squeeze(1)\n",
    "            metrics = calc_metrics(pred, mask)\n",
    "        tot_loss += loss.item()\n",
    "        for k in tot_metrics: tot_metrics[k] += metrics[k]\n",
    "        n += 1\n",
    "    avg_loss = tot_loss / n\n",
    "    avg_metrics = {k: tot_metrics[k]/n for k in tot_metrics}\n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/406 [00:00<?, ?it/s]C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_46524\\692701976.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4424  mIoU: 0.6369  mDice: 0.7656  F1: 0.7656\n",
      "Val   Loss: 0.3647  mIoU: 0.7067  mDice: 0.7959  F1: 0.7959\n",
      "✅ Saved new best model (val mIoU=0.7067)\n",
      "\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4182  mIoU: 0.6661  mDice: 0.7830  F1: 0.7830\n",
      "Val   Loss: 0.3421  mIoU: 0.7291  mDice: 0.8146  F1: 0.8146\n",
      "✅ Saved new best model (val mIoU=0.7291)\n",
      "\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3802  mIoU: 0.6997  mDice: 0.8092  F1: 0.8092\n",
      "Val   Loss: 0.3634  mIoU: 0.7201  mDice: 0.8017  F1: 0.8017\n",
      "\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3786  mIoU: 0.7000  mDice: 0.8089  F1: 0.8089\n",
      "Val   Loss: 0.3545  mIoU: 0.7308  mDice: 0.8065  F1: 0.8065\n",
      "✅ Saved new best model (val mIoU=0.7308)\n",
      "\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3593  mIoU: 0.7126  mDice: 0.8193  F1: 0.8193\n",
      "Val   Loss: 0.3273  mIoU: 0.7307  mDice: 0.8124  F1: 0.8124\n",
      "\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3552  mIoU: 0.7216  mDice: 0.8229  F1: 0.8229\n",
      "Val   Loss: 0.3010  mIoU: 0.7398  mDice: 0.8142  F1: 0.8142\n",
      "✅ Saved new best model (val mIoU=0.7398)\n",
      "\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3413  mIoU: 0.7315  mDice: 0.8304  F1: 0.8304\n",
      "Val   Loss: 0.2974  mIoU: 0.7431  mDice: 0.8235  F1: 0.8235\n",
      "✅ Saved new best model (val mIoU=0.7431)\n",
      "\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3284  mIoU: 0.7403  mDice: 0.8375  F1: 0.8375\n",
      "Val   Loss: 0.2802  mIoU: 0.7576  mDice: 0.8287  F1: 0.8287\n",
      "✅ Saved new best model (val mIoU=0.7576)\n",
      "\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3290  mIoU: 0.7354  mDice: 0.8336  F1: 0.8336\n",
      "Val   Loss: 0.3030  mIoU: 0.7502  mDice: 0.8216  F1: 0.8216\n",
      "\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3125  mIoU: 0.7510  mDice: 0.8430  F1: 0.8430\n",
      "Val   Loss: 0.2835  mIoU: 0.7483  mDice: 0.8243  F1: 0.8243\n",
      "\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3025  mIoU: 0.7602  mDice: 0.8526  F1: 0.8526\n",
      "Val   Loss: 0.2932  mIoU: 0.7527  mDice: 0.8195  F1: 0.8195\n",
      "\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2948  mIoU: 0.7655  mDice: 0.8553  F1: 0.8553\n",
      "Val   Loss: 0.2578  mIoU: 0.7749  mDice: 0.8404  F1: 0.8404\n",
      "✅ Saved new best model (val mIoU=0.7749)\n",
      "\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2880  mIoU: 0.7695  mDice: 0.8573  F1: 0.8573\n",
      "Val   Loss: 0.2754  mIoU: 0.7648  mDice: 0.8341  F1: 0.8341\n",
      "\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2841  mIoU: 0.7742  mDice: 0.8608  F1: 0.8608\n",
      "Val   Loss: 0.2814  mIoU: 0.7562  mDice: 0.8250  F1: 0.8250\n",
      "\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2753  mIoU: 0.7836  mDice: 0.8672  F1: 0.8672\n",
      "Val   Loss: 0.2777  mIoU: 0.7652  mDice: 0.8290  F1: 0.8290\n",
      "\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2727  mIoU: 0.7845  mDice: 0.8666  F1: 0.8666\n",
      "Val   Loss: 0.2829  mIoU: 0.7614  mDice: 0.8270  F1: 0.8270\n",
      "\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2364  mIoU: 0.8210  mDice: 0.8936  F1: 0.8936\n",
      "Val   Loss: 0.2288  mIoU: 0.7891  mDice: 0.8505  F1: 0.8505\n",
      "✅ Saved new best model (val mIoU=0.7891)\n",
      "\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2342  mIoU: 0.8174  mDice: 0.8885  F1: 0.8885\n",
      "Val   Loss: 0.2556  mIoU: 0.7737  mDice: 0.8349  F1: 0.8349\n",
      "\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2124  mIoU: 0.8394  mDice: 0.9057  F1: 0.9057\n",
      "Val   Loss: 0.2633  mIoU: 0.7840  mDice: 0.8421  F1: 0.8421\n",
      "\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2093  mIoU: 0.8415  mDice: 0.9057  F1: 0.9057\n",
      "Val   Loss: 0.2199  mIoU: 0.8009  mDice: 0.8596  F1: 0.8596\n",
      "✅ Saved new best model (val mIoU=0.8009)\n",
      "\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2283  mIoU: 0.8225  mDice: 0.8903  F1: 0.8903\n",
      "Val   Loss: 0.2409  mIoU: 0.7866  mDice: 0.8437  F1: 0.8437\n",
      "\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2125  mIoU: 0.8359  mDice: 0.9032  F1: 0.9032\n",
      "Val   Loss: 0.2332  mIoU: 0.8024  mDice: 0.8597  F1: 0.8597\n",
      "✅ Saved new best model (val mIoU=0.8024)\n",
      "\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2013  mIoU: 0.8462  mDice: 0.9101  F1: 0.9101\n",
      "Val   Loss: 0.2576  mIoU: 0.7813  mDice: 0.8385  F1: 0.8385\n",
      "\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2027  mIoU: 0.8460  mDice: 0.9082  F1: 0.9082\n",
      "Val   Loss: 0.2111  mIoU: 0.7990  mDice: 0.8566  F1: 0.8566\n",
      "\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1970  mIoU: 0.8470  mDice: 0.9088  F1: 0.9088\n",
      "Val   Loss: 0.2264  mIoU: 0.7916  mDice: 0.8488  F1: 0.8488\n",
      "\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1886  mIoU: 0.8556  mDice: 0.9143  F1: 0.9143\n",
      "Val   Loss: 0.2322  mIoU: 0.7758  mDice: 0.8345  F1: 0.8345\n",
      "\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1779  mIoU: 0.8671  mDice: 0.9214  F1: 0.9214\n",
      "Val   Loss: 0.2272  mIoU: 0.7892  mDice: 0.8479  F1: 0.8479\n",
      "\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1699  mIoU: 0.8708  mDice: 0.9255  F1: 0.9255\n",
      "Val   Loss: 0.2156  mIoU: 0.7945  mDice: 0.8516  F1: 0.8516\n",
      "⏹️ Early stopping: validation IoU did not improve for several epochs.\n",
      "\n",
      "Best validation mIoU: 0.8024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train loop with early stopping on val IoU\n",
    "n_epochs = 40  # or higher: stop early if IoU/Dice plateau\n",
    "early_stopper = EarlyStopper(patience=6)\n",
    "best_iou = 0.0\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_iou': [], 'val_iou': [], 'val_f1': [], 'lr': []}\n",
    "model_save_path = 'results/best_unet_oilspill.pth'\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    print(f\"\\nEpoch {epoch}/{n_epochs}\")\n",
    "    train_loss, train_metrics = run_epoch(train_loader, model, criterion, optimizer, scaler, train=True)\n",
    "    val_loss, val_metrics = run_epoch(val_loader, model, criterion, train=False)\n",
    "    scheduler.step(val_metrics['iou'])\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}  mIoU: {train_metrics['iou']:.4f}  mDice: {train_metrics['dice']:.4f}  F1: {train_metrics['f1']:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}  mIoU: {val_metrics['iou']:.4f}  mDice: {val_metrics['dice']:.4f}  F1: {val_metrics['f1']:.4f}\")\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_iou'].append(train_metrics['iou'])\n",
    "    history['val_iou'].append(val_metrics['iou'])\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    improved = early_stopper.step(val_metrics['iou'])\n",
    "    if improved:\n",
    "        best_iou = val_metrics['iou']\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"✅ Saved new best model (val mIoU={best_iou:.4f})\")\n",
    "    if early_stopper.should_stop():\n",
    "        print(\"⏹️ Early stopping: validation IoU did not improve for several epochs.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest validation mIoU: {best_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot IoU, F1, and loss curves\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Val')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(history['train_iou'], label='Train IoU')\n",
    "plt.plot(history['val_iou'], label='Val IoU')\n",
    "plt.title('Mean IoU')\n",
    "plt.legend()\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(history['val_f1'], label='Val F1')\n",
    "plt.title('Val F1')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Model and Evaluate on Validation/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set with all metrics\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    tot_metrics = {'acc':0,'iou':0,'dice':0,'precision':0,'recall':0,'f1':0}\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for img, mask, _ in tqdm(loader, desc='Eval'):\n",
    "            img, mask = img.to(device), mask.to(device)\n",
    "            out = model(img)\n",
    "            prob = torch.sigmoid(out)\n",
    "            pred = (prob > 0.5).long().squeeze(1)\n",
    "            m = calc_metrics(pred, mask)\n",
    "            for k in tot_metrics: tot_metrics[k] += m[k]\n",
    "            n += 1\n",
    "    return {k: tot_metrics[k]/n for k in tot_metrics}\n",
    "\n",
    "val_metrics = evaluate_model(model, val_loader)\n",
    "print(\"Validation metrics:\")\n",
    "for k,v in val_metrics.items(): print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, loader, num_samples=3):\n",
    "    model.eval()\n",
    "    img_batch, mask_batch, names = next(iter(loader))\n",
    "    img_batch, mask_batch = img_batch.to(device), mask_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(img_batch)\n",
    "        prob = torch.sigmoid(out)\n",
    "        pred = (prob > 0.5).long().squeeze(1)\n",
    "    for i in range(min(num_samples, img_batch.size(0))):\n",
    "        img = img_batch[i].cpu().permute(1,2,0).numpy()\n",
    "        img = (img - img.min()) / (img.max()-img.min() + 1e-8)\n",
    "        mask = mask_batch[i].cpu().numpy()\n",
    "        pr = pred[i].cpu().numpy()\n",
    "        plt.figure(figsize=(12,3))\n",
    "        plt.subplot(1,3,1); plt.imshow(img); plt.title('Image'); plt.axis('off')\n",
    "        plt.subplot(1,3,2); plt.imshow(mask, cmap='gray'); plt.title('GT Mask'); plt.axis('off')\n",
    "        plt.subplot(1,3,3); plt.imshow(pr, cmap='gray'); plt.title('Pred Mask'); plt.axis('off')\n",
    "        plt.tight_layout(); plt.show()\n",
    "visualize_predictions(model, val_loader, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- **Stop training if validation IoU or F1 stops improving for >6 epochs (early stopping).**\n",
    "- **IoU > 0.6 and F1 > 0.7** are good targets, but always check qualitative results for false alarms.\n",
    "- If validation metrics still improve, continue training >20 epochs (don't stop arbitrarily at 20).\n",
    "- Acceptable false alarm (FP) rate: **5-10%** (as per mentor advice)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oilspill-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
